/var/spool/slurmd/job652188/slurm_script: line 15: activate: No such file or directory
W0424 04:47:37.700000 2208486 site-packages/torch/distributed/run.py:766] 
W0424 04:47:37.700000 2208486 site-packages/torch/distributed/run.py:766] *****************************************
W0424 04:47:37.700000 2208486 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0424 04:47:37.700000 2208486 site-packages/torch/distributed/run.py:766] *****************************************
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 3): env://, gpu 3
[rank3]:[W424 04:47:52.043810650 ProcessGroupNCCL.cpp:4693] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
[rank2]:[W424 04:47:52.051260982 ProcessGroupNCCL.cpp:4693] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
[rank1]:[W424 04:47:52.051279893 ProcessGroupNCCL.cpp:4693] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
[rank0]:[W424 04:47:52.077445717 ProcessGroupNCCL.cpp:4693] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: averyyy (ebwm_nlp) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /work/hdd/bdta/aqian1/mar_ebwm/wandb/run-20250424_044757-1q73fl7q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mar-base-energy-1k-64-a.01m9000-continue
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ebwm_nlp/ebwm-mar
wandb: üöÄ View run at https://wandb.ai/ebwm_nlp/ebwm-mar/runs/1q73fl7q
[04:47:58.112672] job dir: /work/hdd/bdta/aqian1/mar_ebwm
[04:47:58.113070] Namespace(batch_size=128,
epochs=100,
model='mar_base',
img_size=64,
vae_path='pretrained_models/vae/kl16.ckpt',
vae_embed_dim=16,
vae_stride=16,
patch_size=1,
num_iter=64,
num_images=1000,
cfg=1.0,
cfg_schedule='linear',
label_drop_prob=0.1,
eval_freq=20,
save_last_freq=5,
online_eval=True,
evaluate=False,
eval_bsz=64,
weight_decay=0.02,
grad_checkpointing=False,
lr=None,
blr=0.0003,
min_lr=0.0,
lr_schedule='constant',
warmup_epochs=10,
ema_rate=0.9999,
mask_ratio_min=0.7,
grad_clip=3.0,
attn_dropout=0.1,
proj_dropout=0.1,
buffer_size=64,
diffloss_d=12,
diffloss_w=1536,
num_sampling_steps='100',
diffusion_batch_mul=1,
temperature=1.0,
data_path='./data/imagenet',
class_num=1000,
output_dir='/work/hdd/bdta/aqian1/mar_ebwm/output/mar-base-energy-tiny-alpha-0.01-mult-9000',
log_dir='/work/hdd/bdta/aqian1/mar_ebwm/output/mar-base-energy-tiny-alpha-0.01-mult-9000',
device='cuda',
seed=42,
resume='/work/hdd/bdta/aqian1/mar_ebwm/output/mar-base-energy-tiny-alpha-0.01-mult-9000',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=4,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
use_cached=True,
cached_path='/work/hdd/bdta/aqian1/mar_ebwm/data/cached-imagenet1k-64',
model_type='mar',
run_name='mar-base-energy-1k-64-a.01m9000-continue',
embed_dim=1024,
depth=16,
num_heads=16,
mlp_ratio=4.0,
learn_sigma=False,
mcmc_num_steps=10,
mcmc_step_size=0.01,
langevin_dynamics_noise=0.01,
denoising_initial_condition='random_noise',
grad_accu=4,
mcmc_step_size_lr_multiplier=9000.0,
rank=0,
gpu=0,
distributed=True,
dist_backend='nccl')
[04:48:00.150370] Dataset CachedFolder
    Number of datapoints: 1281153
    Root location: /work/hdd/bdta/aqian1/mar_ebwm/data/cached-imagenet1k-64
[04:48:00.151019] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0xfffb58f92850>
[04:48:00.242510] Working with z of shape (1, 16, 16, 16) = 4096 dimensions.
[04:48:00.841814] Loading pre-trained KL-VAE
[04:48:00.842360] Missing keys:
[04:48:00.842579] []
[04:48:00.842765] Unexpected keys:
[04:48:00.842933] []
[04:48:00.843099] Restored from pretrained_models/vae/kl16.ckpt
[04:48:03.946867] Model = MAR(
  (class_emb): Embedding(1000, 768)
  (z_proj): Linear(in_features=16, out_features=768, bias=True)
  (z_proj_ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (encoder_blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.1, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.1, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop2): Dropout(p=0.1, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (decoder_embed): Linear(in_features=768, out_features=768, bias=True)
  (decoder_blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.1, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.1, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop2): Dropout(p=0.1, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (decoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (energy_mlp): EnergyMLP(
    (input_proj): Linear(in_features=16, out_features=1024, bias=True)
    (cond_proj): Linear(in_features=768, out_features=1024, bias=True)
    (res_blocks): ModuleList(
      (0-2): 3 x AdaLNResBlock(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): SiLU()
          (2): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (adaLN_modulation): Sequential(
          (0): SiLU()
          (1): Linear(in_features=1024, out_features=3072, bias=True)
        )
      )
    )
    (final_layer): Sequential(
      (0): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (1): Linear(in_features=1024, out_features=1, bias=True)
    )
  )
)
[04:48:03.948256] Number of trainable parameters: 188.171778M
[04:48:03.973745] base lr: 3.00e-04
[04:48:03.974083] actual lr: 2.40e-03
[04:48:03.974288] effective batch size: 2048
[04:48:04.021213] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    decoupled_weight_decay: True
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0024
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    decoupled_weight_decay: True
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0024
    maximize: False
    weight_decay: 0.02

Parameter Group 2
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    decoupled_weight_decay: True
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 21.599999999999998
    maximize: False
    weight_decay: 0.0
)
/work/hdd/bdta/aqian1/mar_ebwm/util/misc.py:271: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
/work/hdd/bdta/aqian1/mar_ebwm/util/misc.py:271: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
/work/hdd/bdta/aqian1/mar_ebwm/util/misc.py:271: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
/work/hdd/bdta/aqian1/mar_ebwm/util/misc.py:271: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
[04:48:07.438645] Resume checkpoint /work/hdd/bdta/aqian1/mar_ebwm/output/mar-base-energy-tiny-alpha-0.01-mult-9000
[04:48:07.478362] With optim & sched!
[04:48:07.479446] Start training for 100 epochs
[04:48:07.481606] log_dir: /work/hdd/bdta/aqian1/mar_ebwm/output/mar-base-energy-tiny-alpha-0.01-mult-9000
/work/hdd/bdta/aqian1/mar_ebwm/engine_mar.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/work/hdd/bdta/aqian1/mar_ebwm/engine_mar.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/work/hdd/bdta/aqian1/mar_ebwm/engine_mar.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/work/hdd/bdta/aqian1/mar_ebwm/engine_mar.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
[04:48:14.942413] Epoch: [6]  [   0/2502]  eta: 5:10:59  lr: 0.001440  loss: 0.6859 (0.6859)  mcmc_step_size: 0.0286 (0.0286)  time: 7.4578  data: 6.1745  max mem: 72954
[04:48:24.190518] Epoch: [6]  [  20/2502]  eta: 0:32:52  lr: 0.001442  loss: 0.6801 (0.6825)  mcmc_step_size: 0.0284 (0.0284)  time: 0.4616  data: 0.0902  max mem: 73836
[04:48:34.260935] Epoch: [6]  [  40/2502]  eta: 0:26:46  lr: 0.001444  loss: 0.6942 (0.6871)  mcmc_step_size: 0.0283 (0.0284)  time: 0.5029  data: 0.0682  max mem: 73836
[04:48:43.448846] Epoch: [6]  [  60/2502]  eta: 0:23:58  lr: 0.001446  loss: 0.6992 (0.6907)  mcmc_step_size: 0.0282 (0.0283)  time: 0.4586  data: 0.0001  max mem: 73839
[04:48:52.629499] Epoch: [6]  [  80/2502]  eta: 0:22:28  lr: 0.001448  loss: 0.6932 (0.6910)  mcmc_step_size: 0.0281 (0.0283)  time: 0.4583  data: 0.0001  max mem: 73845
[04:49:00.759410] Epoch: [6]  [ 100/2502]  eta: 0:21:05  lr: 0.001450  loss: 0.6955 (0.6905)  mcmc_step_size: 0.0278 (0.0282)  time: 0.4058  data: 0.0001  max mem: 73845
[04:49:08.897314] Epoch: [6]  [ 120/2502]  eta: 0:20:07  lr: 0.001452  loss: 0.6916 (0.6908)  mcmc_step_size: 0.0275 (0.0281)  time: 0.4061  data: 0.0001  max mem: 73845
[04:49:18.975286] Epoch: [6]  [ 140/2502]  eta: 0:19:55  lr: 0.001453  loss: 0.6960 (0.6913)  mcmc_step_size: 0.0276 (0.0280)  time: 0.5032  data: 0.0746  max mem: 73845
[04:49:27.502003] Epoch: [6]  [ 160/2502]  eta: 0:19:22  lr: 0.001455  loss: 0.6834 (0.6910)  mcmc_step_size: 0.0274 (0.0279)  time: 0.4256  data: 0.0623  max mem: 73845
[04:49:36.242046] Epoch: [6]  [ 180/2502]  eta: 0:18:56  lr: 0.001457  loss: 0.6872 (0.6907)  mcmc_step_size: 0.0273 (0.0279)  time: 0.4363  data: 0.0863  max mem: 73845
[04:49:46.571609] Epoch: [6]  [ 200/2502]  eta: 0:18:53  lr: 0.001459  loss: 0.6973 (0.6915)  mcmc_step_size: 0.0272 (0.0278)  time: 0.5158  data: 0.1878  max mem: 73845
[04:49:55.607007] Epoch: [6]  [ 220/2502]  eta: 0:18:34  lr: 0.001461  loss: 0.6878 (0.6915)  mcmc_step_size: 0.0271 (0.0277)  time: 0.4510  data: 0.0901  max mem: 73845
[04:50:04.344980] Epoch: [6]  [ 240/2502]  eta: 0:18:15  lr: 0.001463  loss: 0.6907 (0.6913)  mcmc_step_size: 0.0269 (0.0277)  time: 0.4362  data: 0.1061  max mem: 73845
[04:50:13.881640] Epoch: [6]  [ 260/2502]  eta: 0:18:04  lr: 0.001465  loss: 0.6864 (0.6917)  mcmc_step_size: 0.0269 (0.0276)  time: 0.4761  data: 0.0836  max mem: 73845
[04:50:23.198165] Epoch: [6]  [ 280/2502]  eta: 0:17:51  lr: 0.001467  loss: 0.6915 (0.6916)  mcmc_step_size: 0.0269 (0.0276)  time: 0.4652  data: 0.0700  max mem: 73845
[04:50:32.037312] Epoch: [6]  [ 300/2502]  eta: 0:17:35  lr: 0.001469  loss: 0.6890 (0.6913)  mcmc_step_size: 0.0265 (0.0275)  time: 0.4413  data: 0.1119  max mem: 73845
[04:50:43.036906] Epoch: [6]  [ 320/2502]  eta: 0:17:35  lr: 0.001471  loss: 0.7012 (0.6920)  mcmc_step_size: 0.0265 (0.0274)  time: 0.5493  data: 0.2123  max mem: 73845
[04:50:51.307982] Epoch: [6]  [ 340/2502]  eta: 0:17:17  lr: 0.001473  loss: 0.6924 (0.6926)  mcmc_step_size: 0.0262 (0.0274)  time: 0.4129  data: 0.0831  max mem: 73845
[04:50:59.659648] Epoch: [6]  [ 360/2502]  eta: 0:17:00  lr: 0.001475  loss: 0.7027 (0.6976)  mcmc_step_size: 0.0270 (0.0273)  time: 0.4169  data: 0.0509  max mem: 73845
[04:51:07.596123] Epoch: [6]  [ 380/2502]  eta: 0:16:41  lr: 0.001476  loss: 0.6881 (0.6972)  mcmc_step_size: 0.0269 (0.0273)  time: 0.3963  data: 0.0014  max mem: 73845
[04:51:14.790621] Epoch: [6]  [ 400/2502]  eta: 0:16:20  lr: 0.001478  loss: 0.6838 (0.6969)  mcmc_step_size: 0.0269 (0.0273)  time: 0.3591  data: 0.0001  max mem: 73845
[04:51:22.068377] Epoch: [6]  [ 420/2502]  eta: 0:16:00  lr: 0.001480  loss: 0.6901 (0.6965)  mcmc_step_size: 0.0271 (0.0273)  time: 0.3632  data: 0.0001  max mem: 73845
[04:51:29.435932] Epoch: [6]  [ 440/2502]  eta: 0:15:42  lr: 0.001482  loss: 0.6964 (0.6964)  mcmc_step_size: 0.0270 (0.0273)  time: 0.3677  data: 0.0001  max mem: 73845
[04:51:36.255885] Epoch: [6]  [ 460/2502]  eta: 0:15:23  lr: 0.001484  loss: 0.6904 (0.6962)  mcmc_step_size: 0.0270 (0.0273)  time: 0.3403  data: 0.0047  max mem: 73845
[04:51:44.056289] Epoch: [6]  [ 480/2502]  eta: 0:15:08  lr: 0.001486  loss: 0.6956 (0.6961)  mcmc_step_size: 0.0268 (0.0273)  time: 0.3893  data: 0.0168  max mem: 73845
[04:51:51.563694] Epoch: [6]  [ 500/2502]  eta: 0:14:54  lr: 0.001488  loss: 0.6945 (0.6961)  mcmc_step_size: 0.0268 (0.0272)  time: 0.3747  data: 0.0451  max mem: 73845
[04:51:59.163999] Epoch: [6]  [ 520/2502]  eta: 0:14:39  lr: 0.001490  loss: 0.6877 (0.6959)  mcmc_step_size: 0.0268 (0.0272)  time: 0.3793  data: 0.0503  max mem: 73845
[04:52:08.002538] Epoch: [6]  [ 540/2502]  eta: 0:14:30  lr: 0.001492  loss: 0.6803 (0.6955)  mcmc_step_size: 0.0267 (0.0272)  time: 0.4412  data: 0.0381  max mem: 73845
[04:52:15.749518] Epoch: [6]  [ 560/2502]  eta: 0:14:18  lr: 0.001494  loss: 0.6860 (0.6952)  mcmc_step_size: 0.0267 (0.0272)  time: 0.3867  data: 0.0556  max mem: 73845
[04:52:23.726019] Epoch: [6]  [ 580/2502]  eta: 0:14:06  lr: 0.001496  loss: 0.6880 (0.6950)  mcmc_step_size: 0.0267 (0.0272)  time: 0.3981  data: 0.0656  max mem: 73845
[04:52:30.803105] Epoch: [6]  [ 600/2502]  eta: 0:13:52  lr: 0.001498  loss: 0.6930 (0.6948)  mcmc_step_size: 0.0266 (0.0271)  time: 0.3531  data: 0.0001  max mem: 73845
[04:52:37.421361] Epoch: [6]  [ 620/2502]  eta: 0:13:36  lr: 0.001499  loss: 0.6894 (0.6948)  mcmc_step_size: 0.0266 (0.0271)  time: 0.3302  data: 0.0001  max mem: 73845
[04:52:44.038107] Epoch: [6]  [ 640/2502]  eta: 0:13:22  lr: 0.001501  loss: 0.6862 (0.6946)  mcmc_step_size: 0.0267 (0.0271)  time: 0.3302  data: 0.0001  max mem: 73845
[04:52:51.330498] Epoch: [6]  [ 660/2502]  eta: 0:13:09  lr: 0.001503  loss: 0.6806 (0.6944)  mcmc_step_size: 0.0266 (0.0271)  time: 0.3639  data: 0.0001  max mem: 73845
[04:53:01.065947] Epoch: [6]  [ 680/2502]  eta: 0:13:04  lr: 0.001505  loss: 0.6838 (0.6942)  mcmc_step_size: 0.0267 (0.0271)  time: 0.4861  data: 0.0141  max mem: 73845
[04:53:08.519727] Epoch: [6]  [ 700/2502]  eta: 0:12:52  lr: 0.001507  loss: 0.6872 (0.6942)  mcmc_step_size: 0.0267 (0.0271)  time: 0.3720  data: 0.0001  max mem: 73845
[04:53:17.086917] Epoch: [6]  [ 720/2502]  eta: 0:12:43  lr: 0.001509  loss: 0.6771 (0.6938)  mcmc_step_size: 0.0267 (0.0271)  time: 0.4276  data: 0.0311  max mem: 73845
[04:53:23.695819] Epoch: [6]  [ 740/2502]  eta: 0:12:30  lr: 0.001511  loss: 0.6922 (0.6938)  mcmc_step_size: 0.0266 (0.0271)  time: 0.3297  data: 0.0001  max mem: 73845
[04:53:30.491356] Epoch: [6]  [ 760/2502]  eta: 0:12:18  lr: 0.001513  loss: 0.6936 (0.6938)  mcmc_step_size: 0.0267 (0.0270)  time: 0.3391  data: 0.0008  max mem: 73845
[04:53:37.999264] Epoch: [6]  [ 780/2502]  eta: 0:12:07  lr: 0.001515  loss: 0.6832 (0.6936)  mcmc_step_size: 0.0267 (0.0270)  time: 0.3748  data: 0.0001  max mem: 73845
[04:53:45.403599] Epoch: [6]  [ 800/2502]  eta: 0:11:56  lr: 0.001517  loss: 0.6926 (0.6935)  mcmc_step_size: 0.0266 (0.0270)  time: 0.3695  data: 0.0001  max mem: 73845
[04:53:53.626882] Epoch: [6]  [ 820/2502]  eta: 0:11:47  lr: 0.001519  loss: 0.6876 (0.6936)  mcmc_step_size: 0.0264 (0.0270)  time: 0.4105  data: 0.0001  max mem: 73845
[04:54:01.646063] Epoch: [6]  [ 840/2502]  eta: 0:11:38  lr: 0.001521  loss: 0.6757 (0.6933)  mcmc_step_size: 0.0264 (0.0270)  time: 0.4002  data: 0.0001  max mem: 73845
[04:54:09.280962] Epoch: [6]  [ 860/2502]  eta: 0:11:28  lr: 0.001522  loss: 0.6887 (0.6932)  mcmc_step_size: 0.0264 (0.0270)  time: 0.3810  data: 0.0001  max mem: 73845
[04:54:19.411242] Epoch: [6]  [ 880/2502]  eta: 0:11:23  lr: 0.001524  loss: 0.6786 (0.6930)  mcmc_step_size: 0.0266 (0.0270)  time: 0.5058  data: 0.0001  max mem: 73845
[04:54:26.941680] Epoch: [6]  [ 900/2502]  eta: 0:11:13  lr: 0.001526  loss: 0.6767 (0.6928)  mcmc_step_size: 0.0265 (0.0270)  time: 0.3759  data: 0.0001  max mem: 73845
[04:54:34.057413] Epoch: [6]  [ 920/2502]  eta: 0:11:02  lr: 0.001528  loss: 0.6936 (0.6927)  mcmc_step_size: 0.0263 (0.0269)  time: 0.3551  data: 0.0001  max mem: 73845
[04:54:40.947811] Epoch: [6]  [ 940/2502]  eta: 0:10:52  lr: 0.001530  loss: 0.6925 (0.6926)  mcmc_step_size: 0.0262 (0.0269)  time: 0.3438  data: 0.0001  max mem: 73845
[04:54:47.550622] Epoch: [6]  [ 960/2502]  eta: 0:10:40  lr: 0.001532  loss: 0.6947 (0.6928)  mcmc_step_size: 0.0260 (0.0269)  time: 0.3295  data: 0.0001  max mem: 73845
[04:54:54.297212] Epoch: [6]  [ 980/2502]  eta: 0:10:30  lr: 0.001534  loss: 0.6910 (0.6928)  mcmc_step_size: 0.0258 (0.0269)  time: 0.3367  data: 0.0001  max mem: 73845
[04:55:00.905578] Epoch: [6]  [1000/2502]  eta: 0:10:19  lr: 0.001536  loss: 0.6942 (0.6928)  mcmc_step_size: 0.0257 (0.0269)  time: 0.3298  data: 0.0001  max mem: 73845
[04:55:07.510218] Epoch: [6]  [1020/2502]  eta: 0:10:08  lr: 0.001538  loss: 0.6874 (0.6928)  mcmc_step_size: 0.0254 (0.0268)  time: 0.3295  data: 0.0001  max mem: 73845
[04:55:14.139340] Epoch: [6]  [1040/2502]  eta: 0:09:58  lr: 0.001540  loss: 0.6940 (0.6929)  mcmc_step_size: 0.0252 (0.0268)  time: 0.3307  data: 0.0001  max mem: 73845
[04:55:20.767593] Epoch: [6]  [1060/2502]  eta: 0:09:47  lr: 0.001542  loss: 0.6953 (0.6930)  mcmc_step_size: 0.0253 (0.0268)  time: 0.3307  data: 0.0001  max mem: 73845
[04:55:27.383950] Epoch: [6]  [1080/2502]  eta: 0:09:37  lr: 0.001544  loss: 0.7071 (0.6934)  mcmc_step_size: 0.0250 (0.0267)  time: 0.3301  data: 0.0001  max mem: 73845
[04:55:33.989698] Epoch: [6]  [1100/2502]  eta: 0:09:27  lr: 0.001546  loss: 0.6996 (0.6936)  mcmc_step_size: 0.0249 (0.0267)  time: 0.3297  data: 0.0001  max mem: 73845
[04:55:40.594448] Epoch: [6]  [1120/2502]  eta: 0:09:17  lr: 0.001547  loss: 0.6968 (0.6937)  mcmc_step_size: 0.0249 (0.0267)  time: 0.3297  data: 0.0001  max mem: 73845
[04:55:47.220358] Epoch: [6]  [1140/2502]  eta: 0:09:07  lr: 0.001549  loss: 0.6896 (0.6936)  mcmc_step_size: 0.0248 (0.0267)  time: 0.3306  data: 0.0001  max mem: 73845
[04:55:53.831294] Epoch: [6]  [1160/2502]  eta: 0:08:58  lr: 0.001551  loss: 0.6908 (0.6935)  mcmc_step_size: 0.0245 (0.0266)  time: 0.3298  data: 0.0001  max mem: 73845
[04:56:00.437871] Epoch: [6]  [1180/2502]  eta: 0:08:48  lr: 0.001553  loss: 0.6903 (0.6935)  mcmc_step_size: 0.0244 (0.0266)  time: 0.3298  data: 0.0001  max mem: 73845
[04:56:07.050952] Epoch: [6]  [1200/2502]  eta: 0:08:39  lr: 0.001555  loss: 0.6865 (0.6935)  mcmc_step_size: 0.0244 (0.0265)  time: 0.3300  data: 0.0001  max mem: 73845
[04:56:13.667862] Epoch: [6]  [1220/2502]  eta: 0:08:29  lr: 0.001557  loss: 0.6977 (0.6935)  mcmc_step_size: 0.0243 (0.0265)  time: 0.3301  data: 0.0001  max mem: 73845
[04:56:20.274184] Epoch: [6]  [1240/2502]  eta: 0:08:20  lr: 0.001559  loss: 0.6952 (0.6935)  mcmc_step_size: 0.0242 (0.0265)  time: 0.3297  data: 0.0001  max mem: 73845
[04:56:26.873993] Epoch: [6]  [1260/2502]  eta: 0:08:11  lr: 0.001561  loss: 0.6848 (0.6933)  mcmc_step_size: 0.0241 (0.0264)  time: 0.3295  data: 0.0001  max mem: 73845
[04:56:33.473445] Epoch: [6]  [1280/2502]  eta: 0:08:01  lr: 0.001563  loss: 0.6895 (0.6933)  mcmc_step_size: 0.0241 (0.0264)  time: 0.3293  data: 0.0001  max mem: 73845
[04:56:40.079165] Epoch: [6]  [1300/2502]  eta: 0:07:52  lr: 0.001565  loss: 0.6906 (0.6932)  mcmc_step_size: 0.0241 (0.0264)  time: 0.3296  data: 0.0001  max mem: 73845
[04:56:46.694309] Epoch: [6]  [1320/2502]  eta: 0:07:43  lr: 0.001567  loss: 0.6956 (0.6933)  mcmc_step_size: 0.0241 (0.0263)  time: 0.3300  data: 0.0001  max mem: 73845
[04:56:53.305624] Epoch: [6]  [1340/2502]  eta: 0:07:34  lr: 0.001569  loss: 0.6933 (0.6933)  mcmc_step_size: 0.0241 (0.0263)  time: 0.3299  data: 0.0001  max mem: 73845
[04:56:59.920377] Epoch: [6]  [1360/2502]  eta: 0:07:25  lr: 0.001570  loss: 0.6888 (0.6932)  mcmc_step_size: 0.0241 (0.0263)  time: 0.3300  data: 0.0001  max mem: 73845
[04:57:06.539639] Epoch: [6]  [1380/2502]  eta: 0:07:17  lr: 0.001572  loss: 0.7002 (0.6933)  mcmc_step_size: 0.0240 (0.0262)  time: 0.3302  data: 0.0001  max mem: 73845
[04:57:13.146452] Epoch: [6]  [1400/2502]  eta: 0:07:08  lr: 0.001574  loss: 0.6878 (0.6933)  mcmc_step_size: 0.0238 (0.0262)  time: 0.3296  data: 0.0001  max mem: 73845
[04:57:19.765127] Epoch: [6]  [1420/2502]  eta: 0:06:59  lr: 0.001576  loss: 0.6899 (0.6934)  mcmc_step_size: 0.0237 (0.0262)  time: 0.3303  data: 0.0001  max mem: 73845
[04:57:26.378157] Epoch: [6]  [1440/2502]  eta: 0:06:51  lr: 0.001578  loss: 0.6835 (0.6933)  mcmc_step_size: 0.0236 (0.0261)  time: 0.3301  data: 0.0001  max mem: 73845
[04:57:32.997126] Epoch: [6]  [1460/2502]  eta: 0:06:42  lr: 0.001580  loss: 0.6941 (0.6933)  mcmc_step_size: 0.0237 (0.0261)  time: 0.3303  data: 0.0001  max mem: 73845
[04:57:39.607015] Epoch: [6]  [1480/2502]  eta: 0:06:34  lr: 0.001582  loss: 0.6902 (0.6932)  mcmc_step_size: 0.0237 (0.0261)  time: 0.3299  data: 0.0001  max mem: 73845
[04:57:46.208960] Epoch: [6]  [1500/2502]  eta: 0:06:25  lr: 0.001584  loss: 0.6883 (0.6932)  mcmc_step_size: 0.0237 (0.0260)  time: 0.3294  data: 0.0001  max mem: 73845
[04:57:52.824283] Epoch: [6]  [1520/2502]  eta: 0:06:17  lr: 0.001586  loss: 0.6941 (0.6933)  mcmc_step_size: 0.0235 (0.0260)  time: 0.3300  data: 0.0001  max mem: 73845
[04:57:59.447209] Epoch: [6]  [1540/2502]  eta: 0:06:08  lr: 0.001588  loss: 0.6840 (0.6932)  mcmc_step_size: 0.0236 (0.0260)  time: 0.3304  data: 0.0001  max mem: 73845
[04:58:06.065940] Epoch: [6]  [1560/2502]  eta: 0:06:00  lr: 0.001590  loss: 0.6850 (0.6931)  mcmc_step_size: 0.0236 (0.0259)  time: 0.3303  data: 0.0001  max mem: 73845
[04:58:12.679698] Epoch: [6]  [1580/2502]  eta: 0:05:52  lr: 0.001592  loss: 0.6816 (0.6929)  mcmc_step_size: 0.0234 (0.0259)  time: 0.3300  data: 0.0001  max mem: 73845
[04:58:19.285913] Epoch: [6]  [1600/2502]  eta: 0:05:44  lr: 0.001593  loss: 0.6880 (0.6929)  mcmc_step_size: 0.0234 (0.0259)  time: 0.3296  data: 0.0001  max mem: 73845
[04:58:25.885387] Epoch: [6]  [1620/2502]  eta: 0:05:35  lr: 0.001595  loss: 0.6905 (0.6928)  mcmc_step_size: 0.0231 (0.0258)  time: 0.3293  data: 0.0001  max mem: 73845
[04:58:32.499780] Epoch: [6]  [1640/2502]  eta: 0:05:27  lr: 0.001597  loss: 0.6962 (0.6928)  mcmc_step_size: 0.0230 (0.0258)  time: 0.3300  data: 0.0001  max mem: 73845
[04:58:39.121869] Epoch: [6]  [1660/2502]  eta: 0:05:19  lr: 0.001599  loss: 0.6809 (0.6928)  mcmc_step_size: 0.0229 (0.0258)  time: 0.3304  data: 0.0001  max mem: 73845
[04:58:45.761524] Epoch: [6]  [1680/2502]  eta: 0:05:11  lr: 0.001601  loss: 0.6909 (0.6928)  mcmc_step_size: 0.0229 (0.0257)  time: 0.3313  data: 0.0001  max mem: 73845
[04:58:52.374213] Epoch: [6]  [1700/2502]  eta: 0:05:03  lr: 0.001603  loss: 0.6930 (0.6928)  mcmc_step_size: 0.0230 (0.0257)  time: 0.3300  data: 0.0001  max mem: 73845
[04:58:58.984510] Epoch: [6]  [1720/2502]  eta: 0:04:55  lr: 0.001605  loss: 0.6845 (0.6927)  mcmc_step_size: 0.0230 (0.0257)  time: 0.3298  data: 0.0001  max mem: 73845
[04:59:01.431155] Loss is nan, stopping training
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mmar-base-energy-1k-64-a.01m9000-continue[0m at: [34mhttps://wandb.ai/ebwm_nlp/ebwm-mar/runs/1q73fl7q[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250424_044757-1q73fl7q/logs[0m
[rank0]:[W424 04:59:04.086729189 ProcessGroupNCCL.cpp:1497] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0424 04:59:05.879000 2208486 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2208551 closing signal SIGTERM
W0424 04:59:05.881000 2208486 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2208552 closing signal SIGTERM
W0424 04:59:05.881000 2208486 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2208554 closing signal SIGTERM
E0424 04:59:06.396000 2208486 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 2 (pid: 2208553) of binary: /u/aqian1/.conda/envs/mar_gh200/bin/python3.11
Traceback (most recent call last):
  File "/u/aqian1/.conda/envs/mar_gh200/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/u/aqian1/.conda/envs/mar_gh200/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/u/aqian1/.conda/envs/mar_gh200/lib/python3.11/site-packages/torch/distributed/run.py", line 892, in main
    run(args)
  File "/u/aqian1/.conda/envs/mar_gh200/lib/python3.11/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/u/aqian1/.conda/envs/mar_gh200/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/aqian1/.conda/envs/mar_gh200/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_mar.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-24_04:59:05
  host      : gh070.hsn.cm.delta.internal.ncsa.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 2208553)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
